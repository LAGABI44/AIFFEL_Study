{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52mhDwdRBI9_"
      },
      "source": [
        "# 프로젝트4 : Scikit-learn의 Toy Dataset 활용\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<목차>\n",
        "Step 1. 데이터 다운로드   \n",
        "Step 2. 데이터 읽어오기   \n",
        "Step 3. 데이터 정제   \n",
        "- 특수문자 제거   \n",
        "- 정규표현식 데이터 정제   \n",
        "- [번외]기타 실험   \n",
        "\n",
        "Step 4. 평가 데이터셋 분리   \n",
        "- 토크나이저 생성   \n",
        "- 텐서플로우 활용 데이터 분리   \n",
        "\n",
        "Step 5. 인공지능 만들기   "
      ],
      "metadata": {
        "id": "2AivGxJp_aAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1. 데이터 다운로드"
      ],
      "metadata": {
        "id": "1emLx0ZD5zP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p ~/content/lyricist/models\n",
        "! ln -s ~/data ~/content/lyricist/data"
      ],
      "metadata": {
        "id": "FBu2k5qlmvoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2. 데이터 읽어오기"
      ],
      "metadata": {
        "id": "Ke1SfF6B5-2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIvx7NVjA882",
        "outputId": "d76b79b1-c051-4a62-d0c1-4a217232f846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Can we forget about the things I said when I was drunk...', \"I didn't mean to call you that\", \"I can't remember what was said\"]\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "txt_file_path = '/content/lyricist/data/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3. 데이터 정제\n",
        "###**특수문자 제거**"
      ],
      "metadata": {
        "id": "EVuQaMvO6BBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytMGFbfX2s_W",
        "outputId": "053fb3bc-b2e8-4f32-85f9-d9e0c460f0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ],
      "source": [
        "#정규표현식 정제\n",
        "import os, re \n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
        "    return sentence\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**정규표현식 데이터 정제**"
      ],
      "metadata": {
        "id": "w613rIk3vdH4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIPbCQTuBq4m",
        "outputId": "860de8d7-18fa-4182-8b36-e2df6b56cac8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> the unsuspecting victim of darkness in the valley <end>',\n",
              " '<start> we can live like jack and sally if we want <end>',\n",
              " '<start> where you can always find me <end>',\n",
              " '<start> and we ll have halloween on christmas <end>',\n",
              " '<start> and in the night we ll wish this never ends <end>',\n",
              " '<start> we ll wish this never ends i miss you i miss you <end>',\n",
              " '<start> i miss you i miss you where are you and i m so sorry <end>',\n",
              " '<start> i cannot sleep i cannot dream tonight <end>',\n",
              " '<start> i need somebody and always <end>',\n",
              " '<start> this sick strange darkness <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 정규표현식 데이터 정제\n",
        "\n",
        "corpus = []  #정제한 데이터를 저장하기 위함\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    if preprocessed_sentence.count(' ') > 15 : continue\n",
        "    corpus.append(preprocessed_sentence)\n",
        "\n",
        "    #잘 적용되었는지 확인\n",
        "corpus[140:150]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**[번외]기타 실험**"
      ],
      "metadata": {
        "id": "bKaIfmOlvDl7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIsxgLKn34Y3",
        "outputId": "11366eb2-41e9-4fe9-baa7-1959a9f7ea55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> don t waste your time on me you re already <end>',\n",
              " '<start> the voice inside my head i miss you , miss you <end>',\n",
              " '<start> don t waste your time on me you re already <end>',\n",
              " '<start> the voice inside my head i miss you , miss you <end>',\n",
              " '<start> don t waste your time on me you re already <end>',\n",
              " '<start> the voice inside my head i miss you , miss you <end>',\n",
              " '<start> i miss you , miss you <end>',\n",
              " '<start> i miss you , miss you <end>',\n",
              " '<start> i miss you , miss you <end>',\n",
              " '<start> i miss you , miss you i miss you miss you hello there the angel from my nightmare <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#<번외-비교 대상군>\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "\n",
        "    #잘 적용되었는지 확인 / 단어 15개 넘는 문장 찾는다고 노가다함ㅠㅠ\n",
        "corpus[140:150]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRvECXhJAyjB",
        "outputId": "f7958f61-1582-4228-8dae-2eb1a47526d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> all around you <end>',\n",
              " '<start> that he s in <end>',\n",
              " '<start> to a shadow <end>',\n",
              " '<start> and i wonder <end>',\n",
              " '<start> silvio <end>',\n",
              " '<start> silver and gold <end>',\n",
              " '<start> silvio <end>',\n",
              " '<start> i gotta go <end>',\n",
              " '<start> silvio <end>',\n",
              " '<start> silver and gold <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#<번외-len()1>\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "      #토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외\n",
        "    if sentence[-1] == \":\": continue\n",
        "    if len(sentence) > 15: continue\n",
        "\n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "\n",
        "    #잘 적용되었는지 확인\n",
        "corpus[140:150]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eeo23WMrA761",
        "outputId": "2bc2f841-6c3a-4edb-e445-d3fe5de4fb97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> ok a millie sold first day i went gold how do i celebrate work on the carter <end>',\n",
              " '<start> yup i aint here to brag nor boast this is simply an attempt to thank you <end>',\n",
              " '<start> the most <end>',\n",
              " '<start> you the fan you the man and to my female audience i hope you use sanitizer <end>',\n",
              " '<start> cuz im kissin all ya hands <end>',\n",
              " '<start> all my plans is well executed <end>',\n",
              " '<start> this that electric music you can get electrocuted <end>',\n",
              " '<start> you know i extra do it they say im the best to do <end>',\n",
              " '<start> i say im better than who next to do it or whoever do it <end>',\n",
              " '<start> they could never do it like me i c o n or you could call me mr i go in <end>']"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#<번외-len()2>\n",
        "orpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "      #토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외\n",
        "    if sentence[-1] == \":\": continue\n",
        "    if len(sentence.split()) > 15: continue\n",
        "\n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "\n",
        "    #잘 적용되었는지 확인\n",
        "corpus[140:150]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗ 토큰 15개 이상 컷이 잘 진행되었는지 확인을 위해 실험   \n",
        "1번째부터 139번째까지 단어 15개 이상의 문장이 없어서 노가다로 단어 15개 이상의 문장이 나올 때까지 찾아봄   \n",
        "그리하여 corpus[140:150] 으로 코드 비교를 해봄   \n",
        "처음에는 counter를 사용하여 컷하려고 했으나 아직 사용법이 미숙하여 계속 오류가 남 -> 찾아보았으나 완벽히 이해는 하지 못함   \n",
        "그래서 가장 통상적으로 많이 사용하는 len을 이용함 -> 처음에는 별 문제가 없어보였으나 corpus[140:150]을 통해 문장을 확인해보니 문장이 이상하게 잘림을 발견\n",
        "그래서 여러 방법을 시도한 후 count 로 가장 적합하게 컷을 함."
      ],
      "metadata": {
        "id": "TN_mcD5jvxHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4. 평가 데이터셋 분리\n",
        "### **토크나이저 생성**\n",
        "\n",
        ": 문장을 일정한 기준으로 쪼개는 과정\n",
        "\n",
        "- 문장의 최대길이를 15로 설정\n",
        "- 최대길이가 15보다 작으면 padding값으로 대체(padding=0)\n",
        "- 최대길이가 15보다 크면 뒷 부분 제거"
      ],
      "metadata": {
        "id": "5OAmvgoQ6LiI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvV8UlXJE4QQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly7I13BRBM1V",
        "outputId": "a6a72170-2aec-42bb-fe69-4fdf84de00c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2   4 375 ...   0   0   0]\n",
            " [  2   4  35 ...   0   0   0]\n",
            " [  2 106  39 ...   0   0   0]\n",
            " ...\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7ff47bccb350>\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, #단어장의 개수\n",
        "        filters=' ',    #쓰지 않겠다. 위에서 한 번 정제했기 때문에 필터는 사실상 필요가 없다, 애초에 전처리를 위해서 필요한 아이\n",
        "        oov_token=\"<unk>\"    #12000단어에 포함되지 못한 단어는 '<unk>'로 바꾸기\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYSIwzEKL3vH"
      },
      "source": [
        "### **텐서플로우 활용 데이터 분리**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L554BlqL7SO",
        "outputId": "096fec74-0ff0-48c0-d8a9-b8d3453009c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2   4 375  16 251  10 152   7  15   3   0   0   0   0]\n",
            "[  4 375  16 251  10 152   7  15   3   0   0   0   0   0]\n"
          ]
        }
      ],
      "source": [
        "src_input = tensor[:, :-1]   # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
        "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E7OOV_CQb85",
        "outputId": "e4b0619c-24ae-4cae-ca91-d64e84e832cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Train: (128205, 14)\n",
            "Target Train: (128205, 14)\n"
          ]
        }
      ],
      "source": [
        "#train과 Test(val) 분리\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# lms 지시\n",
        "#총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
        "#섞을 이유가 없기 때문에 random_state는 사용하지 않겠다.\n",
        "\n",
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09KmsVHXUckU"
      },
      "source": [
        "만약 학습 데이터 개수가 124960보다 크다면 위 Step 3.의 데이터 정제 과정을 다시 한번 검토해 보시기를 권합니다.\n",
        "\n",
        "ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ\n",
        "일단 넘어가고^^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr8FgwY5VDb-",
        "outputId": "d4e974fd-b748-4d3b-cbe6-ac08634cbbcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 14), (128, 14)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#데이터셋 정의하기, 객체 생성\n",
        "\n",
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
        "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tbDLUQNUjtW"
      },
      "source": [
        "### Step 5. 인공지능 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxewN30YWcWi",
        "outputId": "9aeffac8-e6df-4f15-b08d-66f833e81b11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 14, 12001), dtype=float32, numpy=\n",
              "array([[[ 6.09320341e-05, -4.77933463e-05, -1.98708804e-04, ...,\n",
              "          9.61692349e-05,  5.27091615e-04,  1.54884125e-04],\n",
              "        [-3.23879562e-04, -1.40228440e-05, -2.42250477e-04, ...,\n",
              "          2.30998034e-04,  3.69648478e-04,  1.88377147e-04],\n",
              "        [-6.53113588e-04,  1.28552449e-04,  1.74270725e-04, ...,\n",
              "          2.70215878e-05,  4.71367850e-04,  4.75162204e-04],\n",
              "        ...,\n",
              "        [ 5.77995961e-04, -1.11244014e-03, -3.78005170e-05, ...,\n",
              "         -3.75737553e-03,  1.78359053e-03,  5.32346033e-03],\n",
              "        [ 7.18952739e-04, -1.15444558e-03, -2.99294159e-04, ...,\n",
              "         -3.93847190e-03,  2.26276810e-03,  5.94071904e-03],\n",
              "        [ 8.41124158e-04, -1.14674889e-03, -5.96423750e-04, ...,\n",
              "         -4.06558346e-03,  2.61846068e-03,  6.45671505e-03]],\n",
              "\n",
              "       [[ 6.09320341e-05, -4.77933463e-05, -1.98708804e-04, ...,\n",
              "          9.61692349e-05,  5.27091615e-04,  1.54884125e-04],\n",
              "        [ 3.95007228e-04, -1.74278350e-04, -9.66716150e-04, ...,\n",
              "          4.07032639e-04,  1.09230890e-03,  3.38624726e-04],\n",
              "        [ 1.40219738e-04, -2.62801419e-04, -1.18908775e-03, ...,\n",
              "          1.21955306e-03,  1.54894742e-03,  7.09569547e-04],\n",
              "        ...,\n",
              "        [ 1.53105578e-03, -1.88625891e-05, -1.08762202e-03, ...,\n",
              "          1.15681614e-03, -1.47655711e-03, -6.01324078e-04],\n",
              "        [ 1.41940441e-03, -2.48179451e-04, -9.26789129e-04, ...,\n",
              "          2.38194101e-04, -8.00954236e-04,  3.09313415e-04],\n",
              "        [ 1.27553113e-03, -5.05393953e-04, -8.56436905e-04, ...,\n",
              "         -6.28251466e-04, -8.16315514e-05,  1.35328074e-03]],\n",
              "\n",
              "       [[ 6.09320341e-05, -4.77933463e-05, -1.98708804e-04, ...,\n",
              "          9.61692349e-05,  5.27091615e-04,  1.54884125e-04],\n",
              "        [ 5.40581939e-04, -4.27273080e-05, -5.21610200e-04, ...,\n",
              "          6.89517299e-04,  6.92250731e-04, -1.66710946e-04],\n",
              "        [ 1.15666550e-03, -2.65168375e-04, -7.92746432e-04, ...,\n",
              "          6.47213077e-04,  9.91295441e-04, -3.42642306e-04],\n",
              "        ...,\n",
              "        [ 5.63496782e-04, -1.31948749e-04,  8.55457271e-04, ...,\n",
              "          9.15091368e-04,  1.34459551e-05, -1.59386382e-03],\n",
              "        [ 5.12471364e-04,  4.31981287e-04,  8.27689713e-04, ...,\n",
              "          4.93288215e-04, -2.92932760e-04, -1.34242803e-03],\n",
              "        [ 5.24346367e-04,  4.57354559e-04,  6.98094314e-04, ...,\n",
              "         -1.39998001e-05, -9.08618240e-05, -6.61036000e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 6.09320341e-05, -4.77933463e-05, -1.98708804e-04, ...,\n",
              "          9.61692349e-05,  5.27091615e-04,  1.54884125e-04],\n",
              "        [ 2.06513741e-05, -1.07105894e-04, -3.41890467e-04, ...,\n",
              "          1.97212445e-04,  8.22466624e-04,  3.15858138e-04],\n",
              "        [ 2.84828217e-04, -3.56000091e-04, -3.36840953e-04, ...,\n",
              "          6.65159430e-04,  1.15758367e-03, -5.55090628e-05],\n",
              "        ...,\n",
              "        [ 1.69368228e-03, -9.67143278e-04, -1.65626570e-03, ...,\n",
              "          8.84101173e-05,  9.22352134e-04,  3.11430078e-03],\n",
              "        [ 1.68650178e-03, -1.17851910e-03, -1.72129122e-03, ...,\n",
              "         -4.94530017e-04,  1.43239042e-03,  3.93014634e-03],\n",
              "        [ 1.64860010e-03, -1.32156513e-03, -1.77887653e-03, ...,\n",
              "         -1.04374543e-03,  1.87925808e-03,  4.73338831e-03]],\n",
              "\n",
              "       [[ 6.09320341e-05, -4.77933463e-05, -1.98708804e-04, ...,\n",
              "          9.61692349e-05,  5.27091615e-04,  1.54884125e-04],\n",
              "        [ 2.96893326e-04, -3.16455262e-04, -3.68787878e-05, ...,\n",
              "          5.82532433e-04,  1.85487195e-04,  4.56742593e-04],\n",
              "        [ 1.99067217e-04, -1.70434156e-04, -5.02884213e-04, ...,\n",
              "          8.33724567e-04,  3.01521504e-06,  5.04457421e-05],\n",
              "        ...,\n",
              "        [ 1.52962544e-04, -1.03938056e-03, -3.55086173e-04, ...,\n",
              "          1.24350248e-03, -1.12585595e-03,  1.71471416e-04],\n",
              "        [ 1.22489975e-04, -1.13454298e-03, -2.60758534e-04, ...,\n",
              "          6.06099959e-04, -5.16988919e-04,  1.08455867e-03],\n",
              "        [ 1.43232712e-04, -1.20920513e-03, -2.78205582e-04, ...,\n",
              "         -5.03118827e-05,  1.80388175e-04,  2.08377838e-03]],\n",
              "\n",
              "       [[ 6.09320341e-05, -4.77933463e-05, -1.98708804e-04, ...,\n",
              "          9.61692349e-05,  5.27091615e-04,  1.54884125e-04],\n",
              "        [ 1.10383051e-04,  9.27247129e-06, -2.31858037e-04, ...,\n",
              "         -2.22134317e-04,  6.67388900e-04,  8.33221944e-04],\n",
              "        [-6.35085817e-05, -1.50187509e-04,  8.79825602e-05, ...,\n",
              "         -2.22939183e-04,  9.01460240e-04,  9.89445020e-04],\n",
              "        ...,\n",
              "        [ 1.06107770e-03,  1.70784164e-03,  5.13076026e-04, ...,\n",
              "          1.42936246e-03,  1.02730386e-03, -1.30875065e-04],\n",
              "        [ 1.24843384e-03,  1.67277444e-03,  5.21700422e-04, ...,\n",
              "          9.70683119e-04,  1.36470294e-03,  4.89205529e-04],\n",
              "        [ 1.36896432e-03,  1.40497496e-03,  4.47408820e-04, ...,\n",
              "          4.16519615e-04,  1.80619326e-03,  1.32606435e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 1024    #단어 하나의 특징 수\n",
        "hidden_size = 1024    #퍼셉트론의 개수\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "\n",
        "\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcb2C-e8Y6Nc",
        "outputId": "c3da1377-a29a-4220-8847-b2002727ecef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     multiple                  12289024  \n",
            "                                                                 \n",
            " lstm_16 (LSTM)              multiple                  8392704   \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              multiple                  8392704   \n",
            "                                                                 \n",
            " dense_8 (Dense)             multiple                  12301025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,375,457\n",
            "Trainable params: 41,375,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy0nTLb1Z3yE",
        "outputId": "9a311b0f-2d46-4232-ec8e-c433c4299655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "251/251 [==============================] - 156s 606ms/step - loss: 2.3093 - val_loss: 2.5962\n",
            "Epoch 2/10\n",
            "251/251 [==============================] - 159s 633ms/step - loss: 2.2174 - val_loss: 2.5835\n",
            "Epoch 3/10\n",
            "251/251 [==============================] - 151s 601ms/step - loss: 2.1552 - val_loss: 2.5713\n",
            "Epoch 4/10\n",
            "251/251 [==============================] - 151s 601ms/step - loss: 2.0977 - val_loss: 2.5628\n",
            "Epoch 5/10\n",
            "251/251 [==============================] - 159s 633ms/step - loss: 2.0426 - val_loss: 2.5578\n",
            "Epoch 6/10\n",
            "251/251 [==============================] - 151s 601ms/step - loss: 1.9896 - val_loss: 2.5499\n",
            "Epoch 7/10\n",
            "251/251 [==============================] - 159s 633ms/step - loss: 1.9385 - val_loss: 2.5458\n",
            "Epoch 8/10\n",
            "251/251 [==============================] - 151s 600ms/step - loss: 1.8891 - val_loss: 2.5406\n",
            "Epoch 9/10\n",
            "251/251 [==============================] - 151s 601ms/step - loss: 1.8407 - val_loss: 2.5374\n",
            "Epoch 10/10\n",
            "251/251 [==============================] - 151s 600ms/step - loss: 1.7932 - val_loss: 2.5320\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5580329f50>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# 첫 번째 시도한 모델 학습\n",
        "# num_words 12000, batch_size=512\n",
        "#embedding_size = 256\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train,epochs=10, batch_size=512, validation_data=(enc_val, dec_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 번째 시도한 모델 학습\n",
        "# num_words 12000, batch_size=256\n",
        "#embedding_size = 256\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train,epochs=10, batch_size=256, validation_data=(enc_val, dec_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QcFclABTihn",
        "outputId": "78712776-b6de-4be4-e38a-5f802820badc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "501/501 [==============================] - 100s 193ms/step - loss: 3.5468 - val_loss: 3.1800\n",
            "Epoch 2/10\n",
            "501/501 [==============================] - 97s 194ms/step - loss: 3.0625 - val_loss: 2.9899\n",
            "Epoch 3/10\n",
            "501/501 [==============================] - 99s 197ms/step - loss: 2.8831 - val_loss: 2.8722\n",
            "Epoch 4/10\n",
            "501/501 [==============================] - 99s 197ms/step - loss: 2.7470 - val_loss: 2.7837\n",
            "Epoch 5/10\n",
            "501/501 [==============================] - 99s 197ms/step - loss: 2.6317 - val_loss: 2.7185\n",
            "Epoch 6/10\n",
            "501/501 [==============================] - 97s 194ms/step - loss: 2.5242 - val_loss: 2.6629\n",
            "Epoch 7/10\n",
            "501/501 [==============================] - 99s 198ms/step - loss: 2.4239 - val_loss: 2.6156\n",
            "Epoch 8/10\n",
            "501/501 [==============================] - 98s 195ms/step - loss: 2.3296 - val_loss: 2.5795\n",
            "Epoch 9/10\n",
            "501/501 [==============================] - 97s 195ms/step - loss: 2.2406 - val_loss: 2.5417\n",
            "Epoch 10/10\n",
            "501/501 [==============================] - 97s 194ms/step - loss: 2.1557 - val_loss: 2.5123\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff47a398ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 세 번째 시도한 모델 학습\n",
        "# num_words 12000, batch_size=128\n",
        "#embedding_size = 256\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train,epochs=10, batch_size=128, validation_data=(enc_val, dec_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9BNkXpzbIVf",
        "outputId": "d11df73b-c2e3-451c-a1cf-8f54a4fc03da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1002/1002 [==============================] - 109s 106ms/step - loss: 3.4206 - val_loss: 3.1017\n",
            "Epoch 2/10\n",
            "1002/1002 [==============================] - 106s 106ms/step - loss: 2.9791 - val_loss: 2.9133\n",
            "Epoch 3/10\n",
            "1002/1002 [==============================] - 107s 106ms/step - loss: 2.7883 - val_loss: 2.7936\n",
            "Epoch 4/10\n",
            "1002/1002 [==============================] - 107s 107ms/step - loss: 2.6361 - val_loss: 2.7075\n",
            "Epoch 5/10\n",
            "1002/1002 [==============================] - 105s 105ms/step - loss: 2.4985 - val_loss: 2.6401\n",
            "Epoch 6/10\n",
            "1002/1002 [==============================] - 105s 105ms/step - loss: 2.3700 - val_loss: 2.5883\n",
            "Epoch 7/10\n",
            "1002/1002 [==============================] - 107s 107ms/step - loss: 2.2486 - val_loss: 2.5471\n",
            "Epoch 8/10\n",
            "1002/1002 [==============================] - 106s 106ms/step - loss: 2.1351 - val_loss: 2.5155\n",
            "Epoch 9/10\n",
            "1002/1002 [==============================] - 106s 106ms/step - loss: 2.0282 - val_loss: 2.4908\n",
            "Epoch 10/10\n",
            "1002/1002 [==============================] - 107s 107ms/step - loss: 1.9282 - val_loss: 2.4774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3f7d8ff50>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 네 번째 시도한 모델 학습\n",
        "# num_words 15000, batch_size=128\n",
        "#embedding_size = 256\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train,epochs=10, batch_size=128, validation_data=(enc_val, dec_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYCiNjZEmHzw",
        "outputId": "a9982873-8485-424a-b650-02cf849c96e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1002/1002 [==============================] - 117s 114ms/step - loss: 3.4107 - val_loss: 3.1122\n",
            "Epoch 2/10\n",
            "1002/1002 [==============================] - 123s 123ms/step - loss: 2.9958 - val_loss: 2.9346\n",
            "Epoch 3/10\n",
            "1002/1002 [==============================] - 114s 114ms/step - loss: 2.8168 - val_loss: 2.8257\n",
            "Epoch 4/10\n",
            "1002/1002 [==============================] - 113s 112ms/step - loss: 2.6769 - val_loss: 2.7475\n",
            "Epoch 5/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.5519 - val_loss: 2.6865\n",
            "Epoch 6/10\n",
            "1002/1002 [==============================] - 113s 113ms/step - loss: 2.4359 - val_loss: 2.6382\n",
            "Epoch 7/10\n",
            "1002/1002 [==============================] - 123s 123ms/step - loss: 2.3278 - val_loss: 2.6004\n",
            "Epoch 8/10\n",
            "1002/1002 [==============================] - 124s 124ms/step - loss: 2.2279 - val_loss: 2.5712\n",
            "Epoch 9/10\n",
            "1002/1002 [==============================] - 124s 124ms/step - loss: 2.1346 - val_loss: 2.5506\n",
            "Epoch 10/10\n",
            "1002/1002 [==============================] - 114s 114ms/step - loss: 2.0468 - val_loss: 2.5373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3f64db4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다섯 번째 시도한 모델 학습\n",
        "# num_words 12000, batch_size=128\n",
        "# 옵티마이저 수정\n",
        "#embedding_size = 128\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train, epochs=10, batch_size=128, validation_data=(enc_val, dec_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRxr_hw3p1ZZ",
        "outputId": "c2636542-5009-421d-a797-cd6d5595aea2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1002/1002 [==============================] - 94s 90ms/step - loss: 85.3905 - val_loss: 60.1697\n",
            "Epoch 2/10\n",
            "1002/1002 [==============================] - 92s 91ms/step - loss: 52.2011 - val_loss: 45.1717\n",
            "Epoch 3/10\n",
            "1002/1002 [==============================] - 90s 90ms/step - loss: 47.8096 - val_loss: 42.3885\n",
            "Epoch 4/10\n",
            "1002/1002 [==============================] - 92s 91ms/step - loss: 44.4562 - val_loss: 55.8165\n",
            "Epoch 5/10\n",
            "1002/1002 [==============================] - 90s 90ms/step - loss: 48.0046 - val_loss: 32.7403\n",
            "Epoch 6/10\n",
            "1002/1002 [==============================] - 92s 91ms/step - loss: 55.3751 - val_loss: 52.3362\n",
            "Epoch 7/10\n",
            "1002/1002 [==============================] - 90s 90ms/step - loss: 52.6024 - val_loss: 67.0664\n",
            "Epoch 8/10\n",
            "1002/1002 [==============================] - 90s 90ms/step - loss: 53.5306 - val_loss: 54.2085\n",
            "Epoch 9/10\n",
            "1002/1002 [==============================] - 90s 90ms/step - loss: 51.0313 - val_loss: 31.8335\n",
            "Epoch 10/10\n",
            "1002/1002 [==============================] - 90s 90ms/step - loss: 51.4197 - val_loss: 53.0465\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3e80dbad0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 여섯 번째 시도한 모델 학습\n",
        "# num_words 12000, batch_size=128\n",
        "# embedding_size = 512\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train,epochs=10, batch_size=128, validation_data=(enc_val, dec_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhVuUuDswqal",
        "outputId": "122703e6-1f6f-4804-c6a4-fcf3c9d8fbf3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1002/1002 [==============================] - 115s 112ms/step - loss: 3.3277 - val_loss: 3.0330\n",
            "Epoch 2/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.9015 - val_loss: 2.8401\n",
            "Epoch 3/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.7029 - val_loss: 2.7215\n",
            "Epoch 4/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.5397 - val_loss: 2.6364\n",
            "Epoch 5/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.3929 - val_loss: 2.5717\n",
            "Epoch 6/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.2566 - val_loss: 2.5195\n",
            "Epoch 7/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.1298 - val_loss: 2.4821\n",
            "Epoch 8/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 2.0117 - val_loss: 2.4535\n",
            "Epoch 9/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 1.9011 - val_loss: 2.4292\n",
            "Epoch 10/10\n",
            "1002/1002 [==============================] - 112s 112ms/step - loss: 1.7974 - val_loss: 2.4164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3951e0a50>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 일곱 번째 시도한 모델 학습\n",
        "# num_words 12000, batch_size=128\n",
        "# embedding_size = 1024\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train,epochs=15, batch_size=128, validation_data=(enc_val, dec_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpiFKp5-4EBJ",
        "outputId": "215862a2-59f2-4f7f-812d-ceeea7814047"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1002/1002 [==============================] - 130s 128ms/step - loss: 3.2690 - val_loss: 2.9727\n",
            "Epoch 2/15\n",
            "1002/1002 [==============================] - 118s 118ms/step - loss: 2.8378 - val_loss: 2.7813\n",
            "Epoch 3/15\n",
            "1002/1002 [==============================] - 118s 118ms/step - loss: 2.6278 - val_loss: 2.6580\n",
            "Epoch 4/15\n",
            "1002/1002 [==============================] - 118s 118ms/step - loss: 2.4426 - val_loss: 2.5676\n",
            "Epoch 5/15\n",
            "1002/1002 [==============================] - 118s 118ms/step - loss: 2.2695 - val_loss: 2.4959\n",
            "Epoch 6/15\n",
            "1002/1002 [==============================] - 119s 119ms/step - loss: 2.1058 - val_loss: 2.4366\n",
            "Epoch 7/15\n",
            "1002/1002 [==============================] - 119s 118ms/step - loss: 1.9511 - val_loss: 2.3969\n",
            "Epoch 8/15\n",
            "1002/1002 [==============================] - 119s 119ms/step - loss: 1.8084 - val_loss: 2.3710\n",
            "Epoch 9/15\n",
            "1002/1002 [==============================] - 119s 119ms/step - loss: 1.6771 - val_loss: 2.3524\n",
            "Epoch 10/15\n",
            "1002/1002 [==============================] - 119s 119ms/step - loss: 1.5589 - val_loss: 2.3469\n",
            "Epoch 11/15\n",
            "1002/1002 [==============================] - 119s 119ms/step - loss: 1.4523 - val_loss: 2.3553\n",
            "Epoch 12/15\n",
            "1002/1002 [==============================] - 129s 129ms/step - loss: 1.3577 - val_loss: 2.3631\n",
            "Epoch 13/15\n",
            "1002/1002 [==============================] - 119s 119ms/step - loss: 1.2755 - val_loss: 2.3830\n",
            "Epoch 14/15\n",
            "1002/1002 [==============================] - 118s 118ms/step - loss: 1.2042 - val_loss: 2.4013\n",
            "Epoch 15/15\n",
            "1002/1002 [==============================] - 118s 117ms/step - loss: 1.1434 - val_loss: 2.4210\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3e98859d0>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗ 밸리데이션 로스를 줄이기 위한 노력은 아래 회고에서 다루겠다."
      ],
      "metadata": {
        "id": "bjB3Omzh2I7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗ 밸리데이션 로스   \n",
        "1) 밸리데이션 로스란?  \n",
        "overfitting을 해결하기 위해 별로도 만들어진 dataset이 아니라, training dataset에서 추출된 가상의 dataset임.\n",
        "\n",
        "2) 만약 밸리데이션 로스가 안 줄어들 땐?   \n",
        "- 데이터 전처리 : 데이터 표준화 및 정규화(배치놈, 스케일링)   \n",
        "- 모델 강제성 : 모델이 너무 복잡하지 않은지 확인, dropout을 추가하고 각 계층의 레이어 수 또는 뉴런 수 줄이기   \n",
        "- 학습 속도 및 감소 속도 : 학습 속도 줄이기\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbhzwwY%2FbtqAtbHdeNA%2FpRBnbKySV9asFqpI1Ozc71%2Fimg.png\" width=\"\" height=\"\"  title=\"px(픽셀) 크기 설정\" alt=\"밸리데이션데이터셋\"></img><br/>\n",
        "[밸리데이션 데이터셋을 이용한 학습 과정]\n",
        "만약 밸리데이션 로스가 증가했다면 학습을 종료한다.   \n",
        "그렇지 않을 경우는 (2)로 돌아가 학습을 계속 진행함   \n",
        "\n",
        "(3)(4)과정을 통해 현재 모델이 학습 과정에서 참조하지 않았던 data를 얼마나 정확하게 예측하는지 평가하고, 이를 학습의 종료 조건으로 이용함으로써 overfitting을 간접적으로 방지\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb7W55F%2FbtqAtCq1C4B%2FifxO1FuK1p0b3LuzvJKbOk%2Fimg.png\" width=\"\" height=\"\"  title=\"px(픽셀) 크기 설정\" alt=\"밸리데이션로스\"></img><br/>\n",
        "validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고, 이에 따라 학습을 중단한다.   \n",
        " validation dataset 또한 test dataset을 완벽히 표현하지는 못 하기 때문에 validation loss가 최소가 되는 시점이 test loss가 최소가 되는 시점과 정확히 일치하지는 않을 수도 있다."
      ],
      "metadata": {
        "id": "GDmG5fnq2SXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated\n",
        "\n",
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GIFD4zywqRlN",
        "outputId": "95d875b7-9567-42c6-a034-e0ee66dcfafe"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dF0nBe234Co"
      },
      "source": [
        "#회고\n",
        "<정제함수 활용 데이터 구축>\n",
        "\n",
        "토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외 코드를 짜줄 때 여러 오류가 발생했다\n",
        "\n",
        "1. if len(sentence.count()) >15: continue\n",
        "\n",
        "  => TypeError: count() takes at least 1 argument (0 given)\n",
        "\n",
        "  인수가 필요하다길래 count() 안에 corpus 를 넣어주었다.\n",
        "2. if len(sentence.count(corpus)) >15: continue\n",
        "\n",
        "  => TypeError: must be str, not list\n",
        "\n",
        "여기서 아직도 개념이 잘 잡히지 않은 것 같아 파이썬 '인수'가 무엇인가? \n",
        "  => 함수를 정의할 때 넣는 값(변수)를 매개변수라 하고, 함수를 호출할 때 넣는 값을 인수라 한다.\n",
        "\n",
        "  여기서 감이 잡혔다. 함수를 호출할 때 넣는 값을 인수라고 하는데 그럼 해당 부분은 preprocessed_sentence가 인수이니까\n",
        "\n",
        "  3. if len(preprocessed_sentence.count( )) >15: continue\n",
        "  \n",
        "  으음... 이것도 해결이 안되는거 같은데 count 함수는 못 쓰나..?\n",
        "\n",
        "\n",
        "4. from collections import Counter    #count 사용을 위해 import 하기\n",
        "\n",
        "5. if Counter(sentence) >15: continue\n",
        "  => TypeError: '>' not supported between instances of 'Counter' and 'int'\n",
        "\n",
        "   세상에 부등호를 지원안한다니ㅠㅠ\n",
        "   <= 또는 >= 이렇게 사용해아한다고 한다..\n",
        "\n",
        "   ** counter() 와 count()의 차이\n",
        "\n",
        "###❗❗밸리데이션 로스❗❗\n",
        "이번 모델에서 핵심은 밸리데이션 로스를 2.2 이하로 줄이는 것이다.   \n",
        "이를 줄이기 위해 상당히 애썼는데 한 번 구현할 때마다 시간이 상당히 오래걸려 쉽지 않았다.   \n",
        "처음에 건드린건 배치사이즈였다.\n",
        "\n",
        "- ⭐ Batch size란?   \n",
        "연산 한 번 할때 들어가는 데이터의 크기를 말함.   \n",
        "배치사이즈가 큰 경우(숫자가 작을 경우) 한 번에 처리해야 할 데이터의 양이 많아지기 때문에 학습 속도가 느려짐.  \n",
        "배치사이즈가 작을 경우(숫자가 큼) 적은 데이터로 업데이트가 자주 발생해서 훈련이 불안정함  \n",
        "\n",
        "\n",
        "(1) 처음에는 batch_size=512로 시작하였다. 초반부터 validation loss가 꽤나 좋게 나와 기대했지만 2.5 부근에서 고전을 면하지 못했다. 그래서 batch_size=256 그리고 batch_size=128 로 변경하여 진행해보았다. batch_size=256로 하였을 때 validation loss값이 점점 좋아지는 것이 눈에 띄게 보였다. 하지만 역시나 2.5 부근에서는 validation loss이 거의 제자리 걸음이었다. batch_size=128로 하였을 땐 한 번에 처리해야할 데이터가 1000이상이 되면서 epoch 한번 도는데 상당한 시간이 소요되었다. 하지만 2.5이하의 의미있는 validation loss값을 발견하였고, batch_size는 128로 고정시키고 다른 요소를 건드렸다. 여기서 처음부터 epoch 수치를 건들지 않은 이유는 epoch 반복할 때마다 나오는 validation loss 수치에서 의미있는 하락폭이 나오지 않으면 아무리 epoch를 높여봤자 원하는 수치를 얻기에는 힘들 것이라고 판단하였고, 과접합 문제를 생각해서라도 마지막에 수정하는 것이 좋을 것이라고 생각했다.   \n",
        "(2) 두 번째로 만진 건 num_words 이다. 12000 이상으로 사용하라는 기준이 있었기에 혹시나 학습 데이터가 작아서 생긴 문제지 않을까 싶어 15000으로 늘려 학습시켜보았다. 단어장 개수 15000개가 12000개보다 loss값이 안 좋아 12000개로 고정시켰다.   \n",
        "(3) 세 번째로는 optimizer를 수정해보았다. Adam 에서 SGD로 변경해보았는데 loss값이 갑자기 두자릿수가 나와 다시 Adam으로 돌렸다...\n",
        "- 옵티마이저 Adam\n",
        "Adam 기법은 momentum 기법과 Rmsprop 기법을 혼합한 기법으로 주로 다른 옵티마이저와 성능비교를 해보면 손실값이 가장 많이 떨어지는 것을 확인할 수 있다.\n",
        "- 옵티마이저 SGD\n",
        "확률적 경사하강법으로, SGD는 Adam과 반대로 성능이 가장 좋지 않은 경우를 많이 보여준다. SGD는 손실함수의 기울기가 무작정 낮아지는 방향으로 진행되기 때문에 손실함수의 기울기가 최저값이 되는 값을 찾는데 비효율적인 탐색경로를 지니게 된다.\n",
        "\n",
        "(4) 네 번째로 embedding_size를 수정함. embedding_size 숫자가 늘어날수록 validation loss 값이 더욱 좋아지는 것을 발견. validation loss = 128에서 256, 512, 1024까지 늘려서 학습을 시킴.\n",
        "(5) 마지막으로 epoch 를 15로 늘려서 실험하였으나 이상하게도 validation loss가 점점 떨어지는게 아닌 값이 왔다갔다한다. 그래서 위의 batch_size와 epoch 재수정하여 결과를 도출하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reference\n",
        "1) https://untitledtblog.tistory.com/158   \n",
        "2) https://choosunsick.github.io/post/optimizer_compare/"
      ],
      "metadata": {
        "id": "ENfFr8Ni5MLB"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[E-04]Make a Lyricsht.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPu3VSHihag8B/VYp3lW2yw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}